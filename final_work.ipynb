{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datathon 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Standartization imports\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Package for missing data, although we do not do use it \n",
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error\n",
    "\n",
    "# Base packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trainning and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning = pd.read_csv('trajnimi.csv')\n",
    "testing = pd.read_csv('testimi.csv')\n",
    "\n",
    "# Dropping the url column as it was not needed, errors if not removed further on\n",
    "trainning = trainning.drop(\"url\", axis=1)\n",
    "testing = testing.drop(\"url\", axis=1)\n",
    "\n",
    "trainning, validation = train_test_split(trainning, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing articles with zero words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with 0 words: 862\n",
      "Number of articles with less than 30 words: 867\n"
     ]
    }
   ],
   "source": [
    "zero_word_articles = trainning[trainning['n_tokens_content'] == 0]\n",
    "zero_word_count = len(zero_word_articles)\n",
    "\n",
    "short_articles = trainning[trainning['n_tokens_content'] < 30]\n",
    "short_articles_count = len(short_articles)\n",
    "\n",
    "print(f\"Number of articles with 0 words: {zero_word_count}\")\n",
    "print(f\"Number of articles with less than 30 words: {short_articles_count}\")\n",
    "\n",
    "# Remove articles with less than 15 words\n",
    "trainning = trainning[trainning[\"n_tokens_content\"] >= 15].copy()\n",
    "testing = testing[testing[\"n_tokens_content\"] >= 15].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score outlier check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (27681, 61)\n",
      "Cleaned Shape: (14853, 61)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Function to remove outliers using Z-score for all columns at once\n",
    "def remove_outliers_zscore(df, threshold=3):\n",
    "    # Create a mask of all True values\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    # Update mask for each numeric column\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        z_scores = np.abs(stats.zscore(df[col]))\n",
    "        mask = mask & (z_scores < threshold)\n",
    "    \n",
    "    # Return filtered dataframe\n",
    "    return df[mask]\n",
    "\n",
    "df_cleaned = remove_outliers_zscore(trainning)\n",
    "testing_cleaned = remove_outliers_zscore(testing)\n",
    "print(\"Original Shape:\", trainning.shape)\n",
    "print(\"Cleaned Shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers using Interquartile range (IQR) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = trainning[\"shares\"].quantile(0.25)\n",
    "Q3 = trainning[\"shares\"].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_cutoff = Q3 +1.5 * IQR\n",
    "lower_cutoff = Q1 -1.5 * IQR\n",
    "\n",
    "print(\"The upper cutoff is the value above which observations are considered outliers.\\n\")\n",
    "print(f\"Outlier threshhold: {upper_cutoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating outliers by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Articles removed from training set: 82\n",
      "Remaining training articles: 27599\n",
      "Mean of Shares: 3030.3521478320386\n",
      "Standard Deviation of Shares: 5536.091505690935\n"
     ]
    }
   ],
   "source": [
    "initial_train_size = trainning.shape[0]\n",
    "initial_test_size = testing.shape[0]\n",
    "\n",
    "#trainning = trainning[trainning[\"shares\"] <= 300000].copy() # first change, removing outlier for nr of shares above 300,000\n",
    "trainning = trainning[trainning[\"shares\"] <= 80000].copy()\n",
    "trainning = trainning[trainning[\"n_tokens_content\"] <= 4000].copy()\n",
    "#trainning = trainning[trainning[\"kw_min_max\"] <= 400000] #without this its 5400 score\n",
    "#trainning = trainning[trainning[\"kw_max_max\"] <= 400000] #without this its 5400 score\n",
    "#trainning = trainning[trainning[\"self_reference_max_shares\"] <= 400000] #without this its 5400 score\n",
    "#trainning = trainning[trainning[\"self_reference_max_shares\"] <= 400000] #without this its 5400 score\n",
    "trainning = trainning[trainning[\"n_tokens_content\"] >= 15].copy()\n",
    "testing = testing[testing[\"n_tokens_content\"] >= 15].copy()\n",
    "\n",
    "# had made a mistake before and corrected with this\n",
    "#zero_word_articles = trainning[trainning['n_tokens_content'] == 0]\n",
    "#zero_word_count = len(zero_word_articles)\n",
    "#print(\"zero word articles number:\", zero_word_count)\n",
    "\n",
    "removed_train = initial_train_size - trainning.shape[0]\n",
    "#removed_test = initial_test_size - testing.shape[0]\n",
    "\n",
    "print(f\"\\nArticles removed from training set: {removed_train}\")\n",
    "#print(f\"Articles removed from testing set: {removed_test}\")\n",
    "print(f\"Remaining training articles: {trainning.shape[0]}\")\n",
    "#print(f\"Remaining testing articles: {testing.shape[0]}\")\n",
    "\n",
    "# STEP 2: Split the cleaned training data into training and validation\n",
    "trainning, validation = train_test_split(trainning, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Mean of Shares:\", trainning[\"shares\"].mean())\n",
    "print(\"Standard Deviation of Shares:\", trainning[\"shares\"].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer - Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "standard = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "svd = TruncatedSVD(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree, NearestCentroid\n",
    "from sklearn import tree # decision tree and random forest\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aldo\\Documents\\work\\machine_learning\\aldo_diku\\machine_learning_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aldo\\Documents\\work\\machine_learning\\aldo_diku\\machine_learning_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Best PCA Components   Best Score  \\\n",
      "Linear Regression                     15  5382.440228   \n",
      "Lasso                                 15  5382.449109   \n",
      "Ridge                                 15  5382.440203   \n",
      "Decision Tree                         15  7472.887787   \n",
      "Random Forest                         15  5386.548412   \n",
      "MLP                                   15  5338.470641   \n",
      "GaussianNB                             2  5941.807113   \n",
      "Kernel Ridge                          15  6152.856301   \n",
      "Logistic Regression                    2  5785.659922   \n",
      "SVR                                    2  5674.108931   \n",
      "ElasticNet                            59  5372.240461   \n",
      "Bayesian Ridge                        59  5378.479293   \n",
      "XGBoost                                2  5390.904785   \n",
      "\n",
      "                                                            Best Model  \n",
      "Linear Regression    (StandardScaler(), PCA(n_components=15), Linea...  \n",
      "Lasso                (StandardScaler(), PCA(n_components=15), Lasso())  \n",
      "Ridge                (StandardScaler(), PCA(n_components=15), Ridge())  \n",
      "Decision Tree        (StandardScaler(), PCA(n_components=15), Decis...  \n",
      "Random Forest        (StandardScaler(), PCA(n_components=15), (Deci...  \n",
      "MLP                  (StandardScaler(), PCA(n_components=15), MLPRe...  \n",
      "GaussianNB           (StandardScaler(), PCA(n_components=2), Gaussi...  \n",
      "Kernel Ridge         (StandardScaler(), PCA(n_components=15), Kerne...  \n",
      "Logistic Regression  (StandardScaler(), PCA(n_components=2), Logist...  \n",
      "SVR                     (StandardScaler(), PCA(n_components=2), SVR())  \n",
      "ElasticNet           (StandardScaler(), PCA(n_components=59), Elast...  \n",
      "Bayesian Ridge       (StandardScaler(), PCA(n_components=59), Bayes...  \n",
      "XGBoost              (StandardScaler(), PCA(n_components=2), XGBReg...  \n",
      "\n",
      "Best overall model: MLP with 15 PCA components\n",
      "Best overall score (RMSE): 5338.4706\n"
     ]
    }
   ],
   "source": [
    "# can add more but with testing these have been the most effective\n",
    "# 59 is the column length\n",
    "pca_components = [2, 15, 59]\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Decision Tree\": tree.DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', max_iter=500, random_state=42),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    #\"Gaussian PR\": GaussianProcessRegressor(),\n",
    "    \"Kernel Ridge\": KernelRidge(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVR\": SVR(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"Bayesian Ridge\": BayesianRidge(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "}\n",
    "\n",
    "# saving the best results for each model\n",
    "best_results = {}\n",
    "\n",
    "# Evaluate models\n",
    "for n in pca_components:\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    \n",
    "    # Create pipeline with Normalizer and PCA\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            (\"normalizer\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=n)),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "\n",
    "        # Train and test\n",
    "        pipeline.fit(trainning.drop('shares', axis=1), trainning['shares'])\n",
    "        predictions = pipeline.predict(validation.drop('shares', axis=1))\n",
    "        \n",
    "        score = root_mean_squared_error(validation['shares'], predictions)  # RMSE\n",
    "        \n",
    "        if model_name not in best_results or score < best_results[model_name][1]:\n",
    "            best_results[model_name] = (n, score, pipeline)\n",
    "\n",
    "best_results_df = pd.DataFrame.from_dict(best_results, orient='index', columns=[\"Best PCA Components\", \"Best Score\", \"Best Model\"])\n",
    "print(best_results_df)\n",
    "\n",
    "# model with the lowest score\n",
    "best_model_name = min(best_results.items(), key=lambda x: x[1][1])[0]\n",
    "best_score = best_results[best_model_name][1]\n",
    "best_pca = best_results[best_model_name][0]\n",
    "\n",
    "print(f\"\\nBest overall model: {best_model_name} with {best_pca} PCA components\")\n",
    "print(f\"Best overall score (RMSE): {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tunning, Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search for: lasso_none_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for lasso_none_none: -29209355.2711\n",
      "Best parameters: {'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_none_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for ridge_none_none: -36421922.6010\n",
      "Best parameters: {'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_none_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_none_pca_components_2-15-60: -29226195.2516\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_none_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_none_pca_components_2-15-60: -29408964.3166\n",
      "Best parameters: {'dim_reduction__n_components': 15, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_none_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_none_svd_components_2-15-60: -29205966.8461\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_none_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_none_svd_components_2-15-60: -29422608.0997\n",
      "Best parameters: {'dim_reduction__n_components': 15, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_standardize_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for lasso_standardize_none: -32579212.2549\n",
      "Best parameters: {'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_standardize_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for ridge_standardize_none: -364949611898102743040.0000\n",
      "Best parameters: {'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_standardize_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_standardize_pca_components_2-15-60: -192448844087209492480.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(1.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_standardize_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_standardize_pca_components_2-15-60: -364949611898101039104.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_standardize_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_standardize_svd_components_2-15-60: -192448844086902718464.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(1.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_standardize_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_standardize_svd_components_2-15-60: -364949611898101956608.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_standardize_lda_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_standardize_lda_components_2-15-60: -364949402237844848640.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_standardize_lda_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_standardize_lda_components_2-15-60: -364731952559613345792.0000\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_normalize_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for lasso_normalize_none: -30289075.6726\n",
      "Best parameters: {'model__alpha': np.float64(0.01)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_normalize_none\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score for ridge_normalize_none: -30288595.8376\n",
      "Best parameters: {'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_normalize_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_normalize_pca_components_2-15-60: -30289569.8239\n",
      "Best parameters: {'dim_reduction__n_components': 15, 'model__alpha': np.float64(0.01)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_normalize_pca_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_normalize_pca_components_2-15-60: -30288595.8376\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_normalize_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_normalize_svd_components_2-15-60: -30290099.6814\n",
      "Best parameters: {'dim_reduction__n_components': 15, 'model__alpha': np.float64(0.01)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_normalize_svd_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_normalize_svd_components_2-15-60: -30288595.8376\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(0.0001)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: lasso_normalize_lda_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for lasso_normalize_lda_components_2-15-60: -30658388.7383\n",
      "Best parameters: {'dim_reduction__n_components': 2, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "Running grid search for: ridge_normalize_lda_components_2-15-60\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score for ridge_normalize_lda_components_2-15-60: -30666922.5547\n",
      "Best parameters: {'dim_reduction__n_components': 2, 'model__alpha': np.float64(10.0)}\n",
      "--------------------------------------------------\n",
      "\n",
      "Ranked Models:\n",
      "1. lasso_none_svd_components_2-15-60: -29205966.8461\n",
      "2. lasso_none_none: -29209355.2711\n",
      "3. lasso_none_pca_components_2-15-60: -29226195.2516\n",
      "4. ridge_none_pca_components_2-15-60: -29408964.3166\n",
      "5. ridge_none_svd_components_2-15-60: -29422608.0997\n",
      "6. ridge_normalize_none: -30288595.8376\n",
      "7. ridge_normalize_svd_components_2-15-60: -30288595.8376\n",
      "8. ridge_normalize_pca_components_2-15-60: -30288595.8376\n",
      "9. lasso_normalize_none: -30289075.6726\n",
      "10. lasso_normalize_pca_components_2-15-60: -30289569.8239\n",
      "11. lasso_normalize_svd_components_2-15-60: -30290099.6814\n",
      "12. lasso_normalize_lda_components_2-15-60: -30658388.7383\n",
      "13. ridge_normalize_lda_components_2-15-60: -30666922.5547\n",
      "14. lasso_standardize_none: -32579212.2549\n",
      "15. ridge_none_none: -36421922.6010\n",
      "16. lasso_standardize_svd_components_2-15-60: -192448844086902718464.0000\n",
      "17. lasso_standardize_pca_components_2-15-60: -192448844087209492480.0000\n",
      "18. ridge_standardize_lda_components_2-15-60: -364731952559613345792.0000\n",
      "19. lasso_standardize_lda_components_2-15-60: -364949402237844848640.0000\n",
      "20. ridge_standardize_pca_components_2-15-60: -364949611898101039104.0000\n",
      "21. ridge_standardize_svd_components_2-15-60: -364949611898101956608.0000\n",
      "22. ridge_standardize_none: -364949611898102743040.0000\n",
      "\n",
      "Overall best model: lasso_none_svd_components_2-15-60\n",
      "Best score: -29205966.8461\n",
      "Best parameters: {'dim_reduction__n_components': 60, 'model__alpha': np.float64(10.0)}\n",
      "\n",
      "Final Model Evaluation on Training Data:\n",
      "R² Score: 0.0372\n",
      "RMSE: 5382.5597\n",
      "\n",
      "Summary of all model configurations:\n",
      "                                       Model    Best Score  \\\n",
      "4          lasso_none_svd_components_2-15-60 -2.920597e+07   \n",
      "0                            lasso_none_none -2.920936e+07   \n",
      "2          lasso_none_pca_components_2-15-60 -2.922620e+07   \n",
      "3          ridge_none_pca_components_2-15-60 -2.940896e+07   \n",
      "5          ridge_none_svd_components_2-15-60 -2.942261e+07   \n",
      "19    ridge_normalize_svd_components_2-15-60 -3.028860e+07   \n",
      "15                      ridge_normalize_none -3.028860e+07   \n",
      "17    ridge_normalize_pca_components_2-15-60 -3.028860e+07   \n",
      "14                      lasso_normalize_none -3.028908e+07   \n",
      "16    lasso_normalize_pca_components_2-15-60 -3.028957e+07   \n",
      "18    lasso_normalize_svd_components_2-15-60 -3.029010e+07   \n",
      "20    lasso_normalize_lda_components_2-15-60 -3.065839e+07   \n",
      "21    ridge_normalize_lda_components_2-15-60 -3.066692e+07   \n",
      "6                     lasso_standardize_none -3.257921e+07   \n",
      "1                            ridge_none_none -3.642192e+07   \n",
      "10  lasso_standardize_svd_components_2-15-60 -1.924488e+20   \n",
      "8   lasso_standardize_pca_components_2-15-60 -1.924488e+20   \n",
      "13  ridge_standardize_lda_components_2-15-60 -3.647320e+20   \n",
      "12  lasso_standardize_lda_components_2-15-60 -3.649494e+20   \n",
      "9   ridge_standardize_pca_components_2-15-60 -3.649496e+20   \n",
      "11  ridge_standardize_svd_components_2-15-60 -3.649496e+20   \n",
      "7                     ridge_standardize_none -3.649496e+20   \n",
      "\n",
      "                                      Best Parameters  \n",
      "4   {'dim_reduction__n_components': 60, 'model__al...  \n",
      "0                  {'model__alpha': np.float64(10.0)}  \n",
      "2   {'dim_reduction__n_components': 60, 'model__al...  \n",
      "3   {'dim_reduction__n_components': 15, 'model__al...  \n",
      "5   {'dim_reduction__n_components': 15, 'model__al...  \n",
      "19  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "15               {'model__alpha': np.float64(0.0001)}  \n",
      "17  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "14                 {'model__alpha': np.float64(0.01)}  \n",
      "16  {'dim_reduction__n_components': 15, 'model__al...  \n",
      "18  {'dim_reduction__n_components': 15, 'model__al...  \n",
      "20  {'dim_reduction__n_components': 2, 'model__alp...  \n",
      "21  {'dim_reduction__n_components': 2, 'model__alp...  \n",
      "6                  {'model__alpha': np.float64(10.0)}  \n",
      "1                  {'model__alpha': np.float64(10.0)}  \n",
      "10  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "8   {'dim_reduction__n_components': 60, 'model__al...  \n",
      "13  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "12  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "9   {'dim_reduction__n_components': 60, 'model__al...  \n",
      "11  {'dim_reduction__n_components': 60, 'model__al...  \n",
      "7                {'model__alpha': np.float64(0.0001)}  \n",
      "Best MSE: 29205966.8461\n",
      "Best RMSE: 5404.2545\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "def grid_search_regression(X, y, cv=5, scoring='neg_mean_squared_error'):\n",
    "    \"\"\"\n",
    "    Perform grid search for Lasso and Ridge regression with dimensionality reduction\n",
    "    and preprocessing techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Features matrix\n",
    "    y : array-like\n",
    "        Target vector\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    scoring : str, default='neg_mean_squared_error'\n",
    "        Scoring metric for grid search\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing best models and results\n",
    "    \"\"\"\n",
    "    # Get max number of components based on data dimensions\n",
    "    n_samples, n_features = X.shape\n",
    "    max_components = min(n_features, n_samples - 1)\n",
    "    \n",
    "    # Define dimensionality reduction components to try\n",
    "    components = [2, 15, max_components]\n",
    "    components = [c for c in components if c <= max_components]\n",
    "    \n",
    "    # Define preprocessing steps\n",
    "    preprocessors = [\n",
    "        ('none', None),\n",
    "        ('standardize', StandardScaler()),\n",
    "        ('normalize', Normalizer())\n",
    "    ]\n",
    "    \n",
    "    # Define dimensionality reduction techniques\n",
    "    dim_reduction = [\n",
    "        ('none', None),\n",
    "        ('pca', PCA()),\n",
    "        ('svd', TruncatedSVD()),\n",
    "        ('lda', LinearDiscriminantAnalysis())\n",
    "    ]\n",
    "    \n",
    "    # Define models with their parameter grids\n",
    "    models = [\n",
    "        ('lasso', Lasso(max_iter=10000, random_state=42), {\n",
    "            'model__alpha': np.logspace(-4, 1, 6)\n",
    "        }),\n",
    "        ('ridge', Ridge(random_state=42), {\n",
    "            'model__alpha': np.logspace(-4, 1, 6)\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Loop through preprocessing methods\n",
    "    for preproc_name, preprocessor in preprocessors:\n",
    "        # Skip LDA for no preprocessing (might cause numerical issues)\n",
    "        dim_red_options = dim_reduction if preproc_name != 'none' else [d for d in dim_reduction if d[0] != 'lda']\n",
    "        \n",
    "        # Loop through dimensionality reduction techniques\n",
    "        for dim_red_name, dim_reducer in dim_red_options:\n",
    "            # Skip if no dimensionality reduction\n",
    "            if dim_red_name == 'none':\n",
    "                dim_red_params = {}\n",
    "                components_to_try = [None]\n",
    "            else:\n",
    "                # Handle LDA differently (n_components limited by number of classes)\n",
    "                if dim_red_name == 'lda':\n",
    "                    n_classes = len(np.unique(y))\n",
    "                    components_to_try = [min(c, n_classes - 1) for c in components if c < n_classes]\n",
    "                    if not components_to_try:\n",
    "                        continue  # Skip if no valid components for LDA\n",
    "                else:\n",
    "                    components_to_try = components\n",
    "                \n",
    "                dim_red_params = {f'dim_reduction__n_components': components_to_try}\n",
    "            \n",
    "            # Loop through regression models\n",
    "            for model_name, model, model_params in models:\n",
    "                # Create pipeline\n",
    "                steps = []\n",
    "                if preprocessor:\n",
    "                    steps.append(('preprocessor', preprocessor))\n",
    "                if dim_reducer:\n",
    "                    steps.append(('dim_reduction', dim_reducer))\n",
    "                steps.append(('model', model))\n",
    "                \n",
    "                pipeline = Pipeline(steps)\n",
    "                \n",
    "                # Combine parameters\n",
    "                param_grid = {**dim_red_params, **model_params}\n",
    "                \n",
    "                # Create descriptive configuration name\n",
    "                config_name = f\"{model_name}_{preproc_name}_{dim_red_name}\"\n",
    "                if dim_red_name != 'none':\n",
    "                    config_name += f\"_components_{'-'.join(map(str, components_to_try))}\"\n",
    "                \n",
    "                # Perform grid search\n",
    "                print(f\"Running grid search for: {config_name}\")\n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline, param_grid, cv=cv, scoring=scoring, \n",
    "                    n_jobs=-1, verbose=1, return_train_score=True\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    grid_search.fit(X, y)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results[config_name] = {\n",
    "                        'best_params': grid_search.best_params_,\n",
    "                        'best_score': grid_search.best_score_,\n",
    "                        'best_estimator': grid_search.best_estimator_,\n",
    "                        'cv_results': grid_search.cv_results_\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"Best score for {config_name}: {grid_search.best_score_:.4f}\")\n",
    "                    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {config_name}: {str(e)}\")\n",
    "    \n",
    "    # Rank all results and find overall best model\n",
    "    if results:\n",
    "        all_scores = [(name, info['best_score']) for name, info in results.items()]\n",
    "        all_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nRanked Models:\")\n",
    "        for rank, (name, score) in enumerate(all_scores, 1):\n",
    "            print(f\"{rank}. {name}: {score:.4f}\")\n",
    "        \n",
    "        best_model_name = all_scores[0][0]\n",
    "        print(f\"\\nOverall best model: {best_model_name}\")\n",
    "        print(f\"Best score: {results[best_model_name]['best_score']:.4f}\")\n",
    "        print(f\"Best parameters: {results[best_model_name]['best_params']}\")\n",
    "        \n",
    "        # Add overall best model to results\n",
    "        results['overall_best'] = {\n",
    "            'model_name': best_model_name,\n",
    "            'best_estimator': results[best_model_name]['best_estimator'],\n",
    "            'best_score': results[best_model_name]['best_score'],\n",
    "            'best_params': results[best_model_name]['best_params']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Run grid search\n",
    "    results = grid_search_regression(trainning.drop('shares', axis=1), trainning['shares'])\n",
    "    \n",
    "    # Use the best model for prediction\n",
    "    best_model = results['overall_best']['best_estimator']\n",
    "    \n",
    "    # Example prediction\n",
    "    y_pred = best_model.predict(validation.drop('shares', axis=1))\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nFinal Model Evaluation on Training Data:\")\n",
    "    print(f\"R² Score: {r2_score(validation['shares'], y_pred):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(validation['shares'], y_pred)):.4f}\")\n",
    "    \n",
    "    # Generate summary table of results\n",
    "    summary = []\n",
    "    for model_name, info in results.items():\n",
    "        if model_name != 'overall_best':\n",
    "            summary.append({\n",
    "                'Model': model_name,\n",
    "                'Best Score': info['best_score'],\n",
    "                'Best Parameters': str(info['best_params'])\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df = summary_df.sort_values('Best Score', ascending=False)\n",
    "    \n",
    "    print(\"\\nSummary of all model configurations:\")\n",
    "    print(summary_df)\n",
    "\n",
    "    real_mse = -results['overall_best']['best_score']\n",
    "    print(f\"Best MSE: {real_mse:.4f}\")\n",
    "    print(f\"Best RMSE: {np.sqrt(real_mse):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sort results by score (lower RMSE is better)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_results_sorted \u001b[38;5;241m=\u001b[39m \u001b[43mbest_results_df\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort results by score (lower RMSE is better)\n",
    "best_results_sorted = best_results_df.sort_values(by=\"Best Score\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=best_results_sorted[\"Best Score\"], \n",
    "    y=best_results_sorted.index, \n",
    "    palette=\"viridis\"\n",
    ")\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(\"Best RMSE Score (Lower is Better)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Best RMSE Score for Each Model (with Optimal PCA Components)\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Annotate with PCA components\n",
    "for index, value in enumerate(best_results_sorted[\"Best Score\"]):\n",
    "    pca_value = best_results_sorted.iloc[index][\"Best PCA Components\"]\n",
    "    plt.text(value + 0.01, index, f\"PCA: {pca_value}\", va=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruaj_parashikimet(id, parashikimet, emri):\n",
    "    # Ensure both inputs are Pandas Series for safe indexing\n",
    "    id = pd.Series(id).reset_index(drop=True)\n",
    "    parashikimet = pd.Series(parashikimet).reset_index(drop=True)\n",
    "\n",
    "    # Find the minimum length to avoid mismatch\n",
    "    min_length = min(len(id), len(parashikimet))\n",
    "\n",
    "    # Truncate both to the same length\n",
    "    id = id.iloc[:min_length]\n",
    "    parashikimet = parashikimet.iloc[:min_length]\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame({\"id\": id, \"shares\": parashikimet})\n",
    "    df.to_csv(emri, index=False)\n",
    "    print(f\"Predictions saved to {emri} with {min_length} entries.\")\n",
    "\n",
    "# Ensure testing['id'] and predictions have matching lengths before passing them\n",
    "ruaj_parashikimet(testing['id'], predictions, \"parashikimet_reg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
